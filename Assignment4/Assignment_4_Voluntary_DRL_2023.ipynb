{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Voluntary part: Deep Reinforcement learning\n",
    "\n",
    "(MMS131, 2023)\n",
    "\n",
    "Contributors: Jonatan WÃ¥rdh, Mats Granath, and Oleksandr Balabanov\n",
    "\n",
    "### Questions or topics that should be addressed as part of the assignment are marked Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "_This assignment is a continuation of the mandatory Assignment 4, but we will now use a neural network to store the $Q$-matrix. This assignement requires that you learn how to build a basic convolutional neural network using Tensorflow. There is any number of sources for this on the web, so it should not be very difficult. Apart from that, it make take some experimenting with the parameters and your network to get the training to work._ \n",
    "\n",
    "In the previous assignment we found a $Q$-matrix that described the values of taking the action up,down,left or right given the state we were in. The state was simply given by the position of the player who lived on a $20\\times 30$ grid corresponding to $600$ possible states; the state space. The $Q$-matrix was quite small ($600\\times 4$) and we had no problem storing all the values and go through them many times and update until the values converged. However, this smallness of the state space is not expected in general.\n",
    "\n",
    "In this assignment we will include a little twist to the game, which makes the state space size explode. What we will do is to let the fire spread with a certain probability after every turn of the player. For the sake of relative simplicity we will consider a $10\\times 10$ grid. The player has $100$ possible positions but the fire can occupy any grid, or not, meaning that we have $100 \\cdot 2^{100} \\approx 10^{32}$ possible states. This is a huge number and there is no way of even storing this number of $Q$-matrix entries, even less so going through and updating all these values repeatedly. Of course, in principle the problem sounds quite simple. There should be no need to tune $10^{32}$ degrees of freedoms to learn the simple task of just avoiding the fire, but exactly how to capture that intuition in a self learning mathematical framework is less clear. (An alternative could be to use a rule based solution, but here we want the agent to learn the rules itself.) This is where the neural network comes in. As we have seen before in the course, a neural network can learn general features of data. For the present problem it amounts to taking the state as an input and giving the value of the different actions as an output, i.e. we use the network to represent the $Q$-matrix. As you will see you will actually only need a very small number of parameters (compared to $10^{32}$) in your network to solve this problem.\n",
    "\n",
    "The ground breaking paper that popularized deep Q-learning: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "What we need to alter from the previous assignement is to implement the network as the $Q$-function. The update of the $Q$-function: $Q(s,a)\\leftarrow(1-\\alpha)Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a'))$, must then be replaced with the learning step of the network. This step meant that $r+\\gamma\\max_{a'}Q(s',a')$ was the new estimate of $Q(s,a)$, thus using the neural network we will then train with $t=r+\\gamma\\max_{a'}Q(s',a')$ as target for $Q(s,a)$. In practice, the network will have the state $s$ as input, and the 4 movement actions as output, with $t$ the training target for one of these actions.\n",
    "\n",
    "So far everything seems like a quite straight forward generalization of the previous assignment. However, training the neural network for this purpose is a quite tricky business. We will review a few problems and cures below. \n",
    "\n",
    "\n",
    "###  Catastrophic forgetting and experience replay\n",
    "\n",
    "The first problem one might encounter when using the network instead of simply storing all the values of the $Q$-matrix is that of catastrophic forgetting. This is easiest explained with an example; Let's assume that the player has the goal on its left hand side and the cliff on its right hand side. It now takes a step to the left and receive a reward for this, the network is then trained on this situation and learns to correlates some  feature of this state to the action of taking a step to the left. In the next game it might happen so that the player ends up with the goal to the right and the cliff to the left. With the network being trained on this the previous move it might see some common features of these two states and decides to make a move to the left, because it was what it had learned last time. However this time this results in a negative reward and is we now train on this event it is likely that the network erase what it previously learned. In this scenario we might go back and forth between these two events and not learn anything. \n",
    "\n",
    "Note that this could not happen if we had the complete $Q$ matrix entries for every single state, simply because the experiences are disjoint; updating one element of the $Q$-matrix will not effect any other value. However, for the network, training on one state will effect the output of another state. So the very property that make networks good for treating a large state space make them sensitive in this regard.\n",
    "\n",
    "In order to solve this we need to make the network learn to tell the qualitative difference between the two states and actions it apparently thought looked quite similar. To do this we to train on both experiences simultaneously or at least repetively. In practice this is done by setting up a memory in which we store a certain number of the  recent experiences, possibly going back quite far. When we come to the training we draw a random sample from this memory and train on it. This is called _experience replay_.\n",
    "\n",
    "### Policy and target networks\n",
    "\n",
    "Another issue is that the network may become unstable and the training might start to diverge. This problem comes partly from the fact that we use the network in order to predict future rewards, rewards that in the beginning are completely random. This means that the network will learn from its own prediction which can lead to a runaway situation. A cure for this is to use two networks: a _policy_ network and a _target_ network. These networks should have exactly the same architecture, they are just updated in different pace. The policy network determines the action of the player and is the network which is trained in the training step. The target network determines the target of the training, that is predicts $\\max_{a'}Q(s',a')$, and is updated less frequently. The target network is updated by copying the weights of the policy network and assigning them to the target network, so the target network is never trained, it is just a copy of the policy network. By delaying the feedback to the target values in the training step the instability might be avoided.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "You will be provided with a code that implements the grid world game. Your task is to make a suitable network, and find suitable parameters. There is not a lot of coding required. Below follows a brief description of the code.\n",
    "\n",
    "\n",
    "## Code:\n",
    "\n",
    "The code defines a class GridWorld. Classes are a standard tool of object oriented programming languages such as Python. If you want you can read up on classes check [w3schools](https://www.w3schools.com/python/python_classes.asp) for a short version and [The Python tutorial](https://docs.python.org/2/tutorial/classes.html) for a more extensive one, but it should not be necessary to solve the assignment. The main point is that you treat objects (or instances) of that class. You do this by calling the constructor to get an object of the class, e.g. below we write <code>world = GridWorld()</code> to get the object <code>world</code> of the class. Now you can call any function of variable in the class by <code>world.variable</code> or <code>world.function(parameters)</code>.\n",
    "\n",
    "\n",
    "### State representation\n",
    "\n",
    "To represent the state we have used an array consisting of three $10\\times 10$ grid layers. The first specifies the position of the player with a 1 at the position of the player, otherwise zero, and if the player has walked outside the grid all elements are zero. The second layer specifies the position of the fire, with 1 where there is a fire and 0 otherwise. The third layer represents the goal and is fixed at the same position. I.e. <code>state[:,:,0]</code> gives the grid describing the position of the player, <code>state[:,:,1]</code> position of the fire and <code>state[:,:,2]</code> position of the goal.\n",
    "\n",
    "## Hints\n",
    "\n",
    "\n",
    "Below are some hints. _Read these before you start, and come back to them if your code runs but the training doesn't work._ Note that the parameter values stated are not necessarily the best values, it is just there to help you search in an appropriate parameter range.\n",
    "\n",
    "$\\bullet$ Your network should not need more than 100000 trainable parameters (we get a good agent using 15000 parameters). Remember, the output should be four real numbers corresponding to the four action values. (This is not a classification task for which softmax output and cross entropy loss are standard.) We have provided appropriate activation function for the last layer and corresponding loss function. Be careful if you change these.  \n",
    "\n",
    "$\\bullet$ To test the minimal requirements on your network you might want to try to disable the fire spread and perhaps also set <code>gamma = 0</code>, then then the network should learn the rewards of every square in the grid. \n",
    "\n",
    "$\\bullet$ The output diagnostic of q_max and q_min gives an indication whether the network output is reasonable or not. The range of Q-values should be in the range of possible returns.   \n",
    "\n",
    "$\\bullet$ Beware of overfitting. If you train the network too much on a set of experiences it may be difficult to divert it to new data. Not having a too large fraction of the replay memeory as the training batch may help. Adding regularizers <code> kernel_regularizer </code> may also help. You can also change the number of epochs in the training module <code> replay </code>. \n",
    "\n",
    "$\\bullet$ Make sure that use use big enough experience replay buffer. We need experience from quite many games in the past, on the order of 100, how many moves does the player do in each game? Considering this, how big should your memory then be. The <code>batchSize</code> is the number of experiences that are averaged over in each training instance of the network. The larger it is the more stable the training, but it also reduces the stochasticity which is important for good training. (Trial and error may be needed to find a good size.) \n",
    "\n",
    "$\\bullet$ In general <code>gamma</code> needs to be quite large in order for the player to see the goal from far away in the grid. If your training becomes unstable for large <code>gamma</code> you should consider having a bigger memory. You can also try to make the synchronization between policy and target networks less frequent. \n",
    "\n",
    "$\\bullet$ You will probably need to use a decay of <code>epsilon</code>, so that the player start out walking random but start listening more and more to the network. However, make sure that you do not quench <code>epsilon</code> to fast. A good idea might be to study how how the player is progressing as <code>epsilon</code> is lowered. If he never finds the goal when you reduced <code>epsilon</code> significantly, you probably reduces it to fast.\n",
    "\n",
    "$\\bullet$ You can consider whether to start at random position or at a fixed position, <code> random_start </code>, depending on how far you training has progressed. \n",
    "\n",
    "$\\bullet$ It is a good idea to interrupt the training and assess how the learning progresses by studying the state value function. You should start seeing that the state values near the goal become higher.\n",
    "\n",
    "$\\bullet$ You will need to train for a few thousand games. However, you should be able to see progress after 1000 games.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and defining GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Libraries for plotting\n",
    "import matplotlib  \n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits import mplot3d\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# For safe copy of varaibles\n",
    "import copy\n",
    "\n",
    "# Nice way of building a memory, a list with maximum size\n",
    "from collections import deque\n",
    "import itertools\n",
    "\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import the Keras layers etc\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout,BatchNormalization\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Gridworld class\n",
    "class GridWorld:\n",
    "     \n",
    "    ##============ CONSTRUCTOR =============\n",
    "    # This creates the instance of the class and is called by GridWorld().\n",
    "    # The first argument in defining any function refers \n",
    "    # to the objects of the class currently handeled, often called self, but it could be anything.\n",
    "    # Note that when calling the functions these first argument is left out, i.e. you only write GridWorld(), \n",
    "    # the first argument is automatically fed by python.\n",
    "    def __init__(self):\n",
    "        # the size of the grid\n",
    "        self.size = np.array([10,10])\n",
    "        # number of layers in state \n",
    "        self.layers = 3\n",
    "        \n",
    "        # Default starting position and goal\n",
    "        self.start = np.array([1,8])\n",
    "        self.goalpos = np.array([8,1])      \n",
    "        \n",
    "        # rewards, gravel refers to an ordinary step \n",
    "        self.cliff = -100\n",
    "        self.fire = -50\n",
    "        self.goal = 100\n",
    "        self.gravel = -1\n",
    "                \n",
    "        # Default values for network\n",
    "        self.gamma = 0       \n",
    "        # Probability of wind\n",
    "        self.wind = 0\n",
    "        #probability for fire to spread\n",
    "        self.prob_spread = 0\n",
    "        \n",
    "        # Default values for epsilon greedy, not optimal, updated further down!\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.99999999999999999999999999\n",
    "        self.epsilon_min = 0.1\n",
    "        \n",
    "        # Memory, default values, not optimal, updated further down!\n",
    "        self.memory_size = 1 \n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        self.batchSize = 1 \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "##============ CREATE STATES =============\n",
    "    # Constructs the state, random_placement is either True or False. If True the player is placed \n",
    "    # randomly, if False the player is initialized in the starting position\n",
    "    \n",
    "    def make_state(self,random_placement):\n",
    "        \n",
    "        if random_placement:\n",
    "            r_x = np.random.randint(self.size[0]) \n",
    "            r_y = np.random.randint(self.size[1])\n",
    "            # if random = goal keep generating values \n",
    "            while r_x == self.goalpos[0] and r_y == self.goalpos[1]:\n",
    "                r_x = np.random.randint(self.size[0]) \n",
    "                r_y = np.random.randint(self.size[1])\n",
    "        else :\n",
    "            r_x = self.start[0]\n",
    "            r_y = self.start[1]\n",
    "        \n",
    "        # Initialize all values in all layers to zero\n",
    "        state = np.zeros((self.size[0],self.size[1],self.layers))\n",
    "        # we will use player_coordinate to keep track of the position of the player\n",
    "        player_coordinate = [0,0]\n",
    "\n",
    "        # Go through all layers and put 1 at the correct position\n",
    "        for x in range(self.size[0]) : \n",
    "            for y in range(self.size[1]) :\n",
    "            \n",
    "                # Player, first layer\n",
    "                if x == r_x and y == r_y :\n",
    "                    state[x,y,0] = 1\n",
    "                    player_coordinate[0] = x\n",
    "                    player_coordinate[1] = y\n",
    "                else : \n",
    "                    state[x,y,0] = 0\n",
    "\n",
    "                       \n",
    "                # Fire, second layer\n",
    "                if (1<= x <=2) and (1<= y <= 2):\n",
    "                    state[x,y,1] = 1    \n",
    "                else :\n",
    "                    state[x,y,1] = 0 \n",
    "            \n",
    "                #Goal, thrid layer\n",
    "                if x == self.goalpos[0] and y == self.goalpos[1] :\n",
    "                    state[x,y,2] = 1\n",
    "                else : \n",
    "                    state[x,y,2] = 0                \n",
    "    \n",
    "        # return state and player_coordinate\n",
    "        return state , player_coordinate \n",
    "    \n",
    "    \n",
    "##============ MAKING MOVES IN GRIDWORLD ================\n",
    " \n",
    "    # This function returns the new state, player position, reward of the move and a variable done\n",
    "    # which tells us if the game is done or not. The arguments are the current state, action, player_coordinate\n",
    "    # and is_wind which takes values True or False and determines if wind should be implemented.\n",
    "    \n",
    "    def make_move(self,state,action,player_coordinate,is_wind):\n",
    "        # Use deepcopy to make a copy of the state, otherwise this would just be a pointer to the same\n",
    "        # object as state. This is an inconvenience with python... \n",
    "        next_state = copy.deepcopy(state)\n",
    "                \n",
    "        if is_wind :\n",
    "            if np.random.rand() < self.wind:\n",
    "                # overwrite action with random action internally\n",
    "                action = np.random.randint(4)\n",
    "                \n",
    "        new_x = player_coordinate[0]\n",
    "        new_y = player_coordinate[1]\n",
    "        \n",
    "        # Assume that the player goes out of the board ,set old position to zero\n",
    "        # and new coordinate to none\n",
    "        next_state[new_x,new_y,0] = 0\n",
    "        next_player_coordinate = None\n",
    "        done = True\n",
    "        reward = self.cliff\n",
    "        \n",
    "        # make move \n",
    "        if action < 2 : # up or down\n",
    "            if action == 0: # up\n",
    "                new_y = new_y + 1\n",
    "            else : # down\n",
    "                new_y = new_y - 1 \n",
    "        else : # left or right\n",
    "            if action == 2: # left \n",
    "                new_x = new_x - 1\n",
    "            else : # right\n",
    "                new_x = new_x + 1   \n",
    "\n",
    "        # If inside grid \n",
    "        if 0<= new_x < self.size[0] and 0<= new_y < self.size[1] :\n",
    "            # if it hits the goal\n",
    "            if state[new_x,new_y,2] == 1:\n",
    "                done = True\n",
    "                reward = self.goal\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            # fire # WHAT IF THE GOAL BURNS?\n",
    "            elif state[new_x,new_y,1] == 1 :\n",
    "                done = False\n",
    "                reward = self.fire\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            # gravel    \n",
    "            else : \n",
    "                done = False \n",
    "                reward = self.gravel\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            \n",
    "        # else, do nothing, next_player coordinate remains None and next_state[:,:,0] remains all zeros\n",
    "            \n",
    "        return next_state, next_player_coordinate , reward , done  \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "#============== SPREADING OF FIRE =====================\n",
    "\n",
    "    # This function takes state and returns, new_state in which the fire has spread. There is \n",
    "    # a possibility that the fire did not spread, therefore it gives did_fire_spread which is True if the \n",
    "    # fire actually did spread, otherwise it is False.\n",
    "    \n",
    "    def let_fire_spread(self,state):\n",
    "        new_state = copy.deepcopy(state)\n",
    "        \n",
    "        # Assume fire did not spread\n",
    "        did_fire_spread = False\n",
    "    \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                # Walk trhough the fire-grid, if encountering fire, see if it spreads\n",
    "                if state[x,y,1] == 1 : \n",
    "                    # with the probability self.prob_spread the fire spreads \n",
    "                    if np.random.rand() < self.prob_spread :\n",
    "                        # a random move , no diagonal moves are allowed\n",
    "                        if np.random.rand() < 0.5 :\n",
    "                            x_step = np.random.randint(2)*2-1\n",
    "                            y_step = 0\n",
    "                        else :\n",
    "                            x_step = 0\n",
    "                            y_step = np.random.randint(2)*2-1\n",
    "                        # if within boundaries\n",
    "                        if (0<= x+x_step < self.size[0]) and (0<= y+y_step < self.size[1]):\n",
    "                            # Check that this square is not allready on fire\n",
    "                            if new_state[x+x_step,y+y_step,1] == 0: \n",
    "                                # LET IT BURN!\n",
    "                                new_state[x+x_step,y+y_step,1] = 1\n",
    "                                did_fire_spread = True\n",
    "                                \n",
    "        \n",
    "        return new_state , did_fire_spread\n",
    "    \n",
    "    \n",
    "    \n",
    "##============ TRAINING AND EXPERIENCE REPLAY ============= \n",
    "  \n",
    "    # Replay implements the training and experience replay. Here we want to show two different implementations\n",
    "    #, one of which is currently commented away in the main loop. The commented version, version 2, we use the\n",
    "    # target network to predict new targets for old states. In version 1, the currently used version,\n",
    "    # we recall the old predictions of the targets. We find that version 1 is more stable\n",
    "    # than version 2 for this problem. But you can try out both versions.\n",
    "    def replay(self, policy_model):\n",
    "\n",
    "        # check if the memory is bigger than the batch size\n",
    "        if len(self.memory) < self.batchSize :\n",
    "            # if not recall whole memory\n",
    "            minibatch = self.memory\n",
    "        else :\n",
    "            # otherwise take a random batch of size batchSize of the memory\n",
    "            minibatch = random.sample(self.memory, self.batchSize)\n",
    "   \n",
    "        # initialize a state and a target batch for training\n",
    "        state_batch = np.zeros((len(minibatch),self.size[0],self.size[1],self.layers))\n",
    "        target_batch = np.zeros((len(minibatch),4))\n",
    "\n",
    "        # Go through memory \n",
    "        i = 0\n",
    "        for (state, q_state, action, reward, next_state, next_q_max, done) in minibatch :\n",
    "            # Version 1:\n",
    "            #target values for network are the same as the output of the network at the time the experience was made\n",
    "            target = q_state.reshape(4) \n",
    "            #except for the action that was actually taken where we have a reward that we can use\n",
    "            new_target = reward         \n",
    "            if not done :\n",
    "                # Version 1:\n",
    "                new_target = reward + self.gamma * next_q_max\n",
    "                \n",
    "            target[action] = new_target\n",
    "            # Put state and target in the training batch\n",
    "            state_batch[i] = state\n",
    "            target_batch[i] = target\n",
    "            i = i + 1\n",
    "                        \n",
    "        # Train, the number of epochs for training can be changed \n",
    "        policy_model.fit(state_batch, target_batch, batch_size = len(minibatch), epochs=5, verbose=2)\n",
    "            \n",
    " \n",
    "            \n",
    "            \n",
    "##====================FOR DISPLAY==============================    \n",
    "\n",
    "    # Used to display the grid\n",
    "    def make_RGB_grid(self,state,path):\n",
    "        grid_RGB = np.ones((self.size[0],self.size[1],3))*0.7 #\n",
    "        \n",
    "        if path is not None :\n",
    "            for i,location in enumerate(path):\n",
    "                grid_RGB[location[0],location[1],:] = np.array([0,0,0]) # black'P' #player\n",
    "    \n",
    "        for x in range(self.size[0]) : \n",
    "            for y in range(self.size[1]) :\n",
    "            \n",
    "                if state[x,y,2]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([245/255,237/255,48/255]) # Yellow\n",
    "                \n",
    "                if state[x,y,1]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([203/255,32/255,40/255]) # Red '-' #pit    \n",
    "   \n",
    "                if state[x,y,0]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([0/255,254/255,0/255]) # Green '-' #pit    \n",
    "   \n",
    "        return grid_RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to set up and compile the Q-network, in Tensorflow and Keras. We have given a skeleton of the network. Fill in and change as you like. You may want to add regularizing, batchnorm layers etc. Input should be the shape of the state matrix, output the 4 action values corresponding to the 4 allowed actions of the agent. The activiation in the final layer and the loss function used are important to get right, and these are already given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "\n",
    "def setup_network(world) : \n",
    "    # setup network, world.size and world.layers give the input_shape\n",
    "    opt=tf.losses.MeanSquaredError()  #loss function \n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3),padding='valid', activation='relu', input_shape=(world.size[0],world.size[1],world.layers)))\n",
    "    \n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # to transition from convolutional to dense layer, you need to Flatten, i.e. transform matrix to 1D array\n",
    "    \n",
    "    model.add(Dense(4, activation='linear'))\n",
    "              \n",
    "    # compile network          \n",
    "    model.compile(loss=opt, optimizer='adam' ,metrics=['mse'])\n",
    "    \n",
    "    # return model \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 4.2.1 Describe briefly the layers in you network and their functionality (1p)\n",
    "What are the input and output dimensions in each layer, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup network and GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GridWorld and Q-networks\n",
    "\n",
    "# Create world from GridWorld\n",
    "world = GridWorld()\n",
    "# Setup network\n",
    "policy_model = setup_network(world)\n",
    "#make a target network as well\n",
    "target_model = setup_network(world)\n",
    "# copy weights from policy to target\n",
    "target_model.set_weights(policy_model.get_weights())\n",
    "\n",
    "# Plot model summary\n",
    "policy_model.summary()\n",
    "\n",
    "#Make state \n",
    "state , player_coordinate = world.make_state(False)\n",
    "\n",
    "# plot it \n",
    "grid_RGB =world.make_RGB_grid(state,None)\n",
    "#\n",
    "fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# We have to invert the x and y axis , go over to numpy array instead\n",
    "plt.imshow(np.swapaxes(np.array(grid_RGB),0,1))\n",
    "#plt.axis('on')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(np.arange(0, world.size[0], dtype=int))\n",
    "plt.yticks(np.arange(0, world.size[1], dtype=int))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network on model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE YOU NEED TO DEFINE PARAMETER VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup system parameters\n",
    "\n",
    "world.gamma = ?\n",
    "world.epsilon_decay = ? #multiplicative factor that reduces epsilon each step, for no reduction use 1\n",
    "world.epsilon = ?  #initial value of epsilon \n",
    "world.wind = 0.1\n",
    "# fire spreading\n",
    "world.prob_spread = 0.5 # \n",
    "\n",
    "#update the target network every \"update_target_network_period game\". Updating target network less often should make\n",
    "#the system more stable, but also convergence slower\n",
    "update_target_network_period = ?\n",
    "\n",
    "#define size of experience replay buffer (how many moves are stored for training) \n",
    "#and batchsize (how many moves from memory buffer are used in each training instance)\n",
    "world.memory_size = ? \n",
    "        \n",
    "world.batchSize = ? \n",
    "\n",
    "world.memory = deque(maxlen=world.memory_size)   #The experience replay memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics. These are used to store the max and min, q-values output by the network for the states visited since the last time the target network was updated. If the training has converged these should correspond quite well to the maximal and minimal rewards available in the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training diagnostics.  \n",
    "q_max = 0\n",
    "q_min = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop. It should be all set to run if you have defined the network and parameters above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MAIN LOOP with network\n",
    "\n",
    "step_count=0;\n",
    "random_start = True  #Easier to train from random start\n",
    "is_wind = False  #Set to false to simplify training \n",
    "next_player_coordinate = None\n",
    "nr_games = 5000  #This is large, you will probably not need as many. Initially make shorter runs to check progress\n",
    "# loop over games\n",
    "for games in range(nr_games):\n",
    "    \n",
    "    # Display\n",
    "    print(\"Game #: %s\" % (games,))\n",
    "    print(\"Epsilon : %7.4f\" % world.epsilon)     \n",
    "    print(\"Step count : %s\" % step_count) \n",
    "    print(\"End pos %s\" % next_player_coordinate)\n",
    "    \n",
    "    # DIAGNOSTICS \n",
    "    print(\"Since updated target: Qmin  %s Qmax %s\" % (q_min,q_max))\n",
    "    \n",
    "        \n",
    "    # reinitize grid every game will be created at start position\n",
    "    state , player_coordinate = world.make_state(random_start)\n",
    "    \n",
    "    step_count=0;\n",
    "    while True :\n",
    "        step_count+=1\n",
    "        \n",
    "        # use policy network to get q\n",
    "        q_state = policy_model.predict(state.reshape(1,world.size[0],world.size[1],world.layers))        \n",
    "        # get best action\n",
    "        action = np.argmax(q_state)   \n",
    "        \n",
    "        # epsilon greedy\n",
    "        if np.random.rand() < world.epsilon :\n",
    "            # take another action\n",
    "            action=np.random.randint(4)\n",
    "            \n",
    "        # make the move\n",
    "        next_state ,next_player_coordinate, reward , done = world.make_move(state,action,player_coordinate,is_wind)\n",
    "                  \n",
    "        # find max q of the next state using target network\n",
    "        next_q_max = np.amax(target_model.predict(next_state.reshape(1,world.size[0],world.size[1],world.layers)))\n",
    "        \n",
    "        # Store in memory\n",
    "        world.memory.extend([(state, q_state, action, reward, next_state, next_q_max, done)])  \n",
    "        \n",
    "        # DIAGNOSTICS UPDATE   =============\n",
    "        if q_max < np.amax(q_state):\n",
    "            q_max = np.amax(q_state)\n",
    "        if q_min > np.amin(q_state):\n",
    "            q_min = np.amin(q_state)\n",
    "            \n",
    "        \n",
    "        #=============================================\n",
    "        \n",
    "          \n",
    "        # break if done or two many steps taken \n",
    "        if done or (step_count > 400): # 10^2 =100 steps to diffuse through the lattice\n",
    "            break\n",
    "        \n",
    "        # update state\n",
    "        state = next_state \n",
    "        player_coordinate = next_player_coordinate \n",
    "\n",
    "        # let fire spread \n",
    "        new_state, fire_spread = world.let_fire_spread(state)\n",
    "        if fire_spread :\n",
    "            state = new_state \n",
    "        \n",
    "    \n",
    "    # end of game, train the network every 10 games. Can be changed. \n",
    "\n",
    "    if (games % 10 == 0 and games>0 ):\n",
    "        \n",
    "        world.replay(policy_model)\n",
    "        \n",
    "        # epsilon decay\n",
    "        if world.epsilon > world.epsilon_min:\n",
    "            world.epsilon *= world.epsilon_decay \n",
    "            \n",
    "    # update target network \n",
    "    if (games % update_target_network_period == 0) :\n",
    "        print(\"Update target network\")\n",
    "        # update the weights of the target model\n",
    "        target_model.set_weights(policy_model.get_weights())\n",
    "        #reset diagnostic\n",
    "        q_max = 0\n",
    "        q_min = 0\n",
    "    \n",
    "    clear_output(wait=True)    \n",
    "    # end of loop\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Kernel as you like after a number of games and check progress by using the two plotfunctions below. Restarting the loop will reset the counter but not reset the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 4.2.2 The main task is to get the training to work and your AI agent to perform. (4p)\n",
    "A good indication is that the q_max and q_min correspond roughly to the goal and cliff rewards. \n",
    "\n",
    "Design and train a Q-networks that solves the game with a fire of probability <code>prob_spread = 0.5</code> and <code>wind = 0</code> for a 10 by 10 grid. Study also the dynamical play of the player. Success implies that the agent (most of the time) manages to move from a random start position to goal (position [8,1]) while (mostly) avoiding the fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, here you could save and load Q-networks to compare\n",
    "policy_model.save('my_network.h1')  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "policy_model = load_model('my_network.h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting : State value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State value function of network, i.e. max over actions of the Q-function\n",
    "\n",
    "# Initialize state, this means that the fire is in the strating position\n",
    "state , player_coordinate = world.make_state(False)\n",
    "\n",
    "# set position to zero\n",
    "state[player_coordinate[0],player_coordinate[1],0] = 0\n",
    "\n",
    "# to plot\n",
    "z= np.zeros((world.size[0],world.size[1]))\n",
    "\n",
    "# Go through all possible position of the player and calculate the value of the best action\n",
    "# according to the network\n",
    "for x in range(world.size[0]) :\n",
    "    for y in range(world.size[1]) :\n",
    "            player_coordinate=[x,y]\n",
    "            state[player_coordinate[0],player_coordinate[1],0] = 1\n",
    "            q_state = policy_model.predict(state.reshape(1,world.size[0],world.size[1],world.layers)).reshape(4)\n",
    "            z[x,y] =q_state.max()\n",
    "            state[x,y,0] = 0\n",
    "        \n",
    "\n",
    "# Plot        \n",
    "plt.figure()\n",
    "fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.imshow(np.swapaxes(z,0,1))\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(np.arange(0, world.size[0], dtype=int))\n",
    "plt.yticks(np.arange(0, world.size[1], dtype=int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 4.2.3 Show that you have trained a network that gives good state values corresponding to a functional agent. (1p)\n",
    "If the dynamic play below works, this should also work. Discuss briefly how one can see that the state value gives a working agent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting : Dynamic play\n",
    "\n",
    "To challenge the agent increase the probability of fire spread, and turn on and increase the wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic game replay\n",
    "\n",
    "for a in range(100):\n",
    "    # fire spreading\n",
    "    world.prob_spread = 0.5\n",
    "    world.wind = 0.1\n",
    "    is_wind = False\n",
    "\n",
    "\n",
    "    #get original state  \n",
    "    state, player_coordinate = world.make_state(True)\n",
    "    path=np.array([player_coordinate])\n",
    "    \n",
    "    # setup figure\n",
    "    fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    done = False\n",
    "    count =0\n",
    "    while (not done) and (count <20) :\n",
    "        count = count + 1\n",
    "        \n",
    "        # plot it \n",
    "        plot_grid = world.make_RGB_grid(state,path)\n",
    "        plt.imshow(np.swapaxes(np.array(plot_grid),0,1))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xticks(np.arange(0, world.size[0], dtype=int))\n",
    "        plt.yticks(np.arange(0, world.size[1], dtype=int))\n",
    "        # clear figure and wait\n",
    "        clear_output(wait=True)\n",
    "        # time.sleep(1)\n",
    "        display(fig)\n",
    "\n",
    "        # find action\n",
    "        q_state = policy_model.predict(state.reshape(1,world.size[0],world.size[1],world.layers))\n",
    "        # get best action, no epsilon greedy\n",
    "        action = np.argmax(q_state)      \n",
    "        # make the move\n",
    "        next_state ,next_player_coordinate, reward , done = world.make_move(state,action,player_coordinate,is_wind)\n",
    "\n",
    "        # update state \n",
    "        state = next_state\n",
    "        player_coordinate = next_player_coordinate\n",
    "\n",
    "        if not done :\n",
    "            path=np.append(path,[player_coordinate],axis = 0)\n",
    "\n",
    "        # let fire spread\n",
    "        new_state, fire_spread = world.let_fire_spread(state)\n",
    "        if fire_spread :\n",
    "            print(\"Fire spread\")\n",
    "            state = new_state \n",
    "\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
